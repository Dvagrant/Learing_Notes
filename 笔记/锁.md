## 全局锁
全局锁就是对整个数据库实例加锁（server层实现）。MySQL 提供了一个加全局读锁的方法，命令是`Flush tables with read lock (FTWRL)`。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。

全局锁的典型使用场景是，做全库逻辑备份。
`FTWRL`确保不会有其他线程对数据库做更新，然后对整个库做备份。注意，在备份过程中整个库完全处于只读状态。但会带来一些问题：
- 在主库上备份，则备份期间所有数据无法更新，服务停摆。
- 在从库上做备份，期间无法执行主库传来的`binlog`，会造成主从延迟。

`set global readonly=true`方式也可以让全库进入只读状态，但我还是会建议你用 FTWRL 方式，主要有两个原因：
- 在有些系统中，`readonly`的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，不建议你使用。
- 在异常处理机制上有差异。如果执行`FTWRL`命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为`readonly`之后，如果客户端发生异常，则数据库就会一直保持`readonly`状态，这样会导致整个库长时间处于不可写状态，风险较高。

业务的更新不只是增删改数据（DML)，还有可能是加字段等修改表结构的操作（DDL）。不论是哪种方法，一个库被全局锁上以后，你要对里面任何一个表做加字段操作，都是会被锁住的。

>官方自带的逻辑备份工具是`mysqldump`。当`mysqldump`使用参数` –single-transaction`的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。`single-transaction`方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过`FTWRL`方法。这往往是 DBA 要求业务开发人员使用`InnoDB`替代`MyISAM`的原因之一。


---
## 表级锁
MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。

表锁（server层实现）的语法是`lock tables … read/write`。与 FTWRL 类似，可以用`unlock tables`主动释放锁，也可以在客户端断开的时候自动释放。需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。
举个例子, 如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。

另一类表级的锁是 MDL（metadata lock)。
MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。

因此，在 MySQL 5.5 版本中引入了 MDL，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。
- 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。
- 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。

给一个表加字段，或者修改字段，或者加索引，需要扫描全表的数据。***索引要根据表中的每一行的记录值来创建，所以需要全表扫描；加字段或修改字段，也要修改每一行记录中的对应列的数据，所以也要全表扫描***

---
## 例子：DDL导致库停止服务
**实验环境：5.6**
![[DDL导致库停止服务.jpg]]

我们可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。

之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。

如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。前面我们说了，所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。（MDL锁会有队列，写锁比读锁优先级高，写锁在排队时后面的读锁也会被阻塞）

如果某个表上的查询语句频繁，而且客户端有重试机制，也就是说超时后会再起一个新 session 再请求的话，这个库的线程很快就会爆满。（MDL锁会在[[事务]]提交时才释放）

>解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 `information\_schema`库的`innodb\_trx`表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。
>
>如果对线上一个频繁DML操作的表做DDL如添加字段等操作，可能会导致死锁，使数据库连接资源被消耗完，导致数据库宕机。安全的解决方式是对表做DDL如添加字段时，设置执行语句的超时时间，写锁超时自动释放，不影响读锁。

>[[INNODB特性#ONLINE DDL|online ddl]]的过程是这样的：  
1\. 拿MDL写锁  
2\. 降级成MDL读锁  
3\. 真正做DDL  
4\. 升级成MDL写锁  
5\. 释放MDL锁


---
## 行锁
MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。

#### 2PL（两阶段锁协议）
加锁解锁过程可分为两个阶段
1. growing phase（此阶段只允许加锁
2. shrinking phase（此阶段只允许解锁

>如果一个transaction释放了它所持有的**任意一个锁**，那它就**再也不能获取任何锁**。
>[Transaction management：两阶段锁（two-phase locking）](https://zhuanlan.zhihu.com/p/59535337)

**如果事务中需要锁多个行，把最有可能造成锁冲突的操作放在最后**

**InnoDB行锁通过锁索引记录实现，若UPDATE的列没有索引，会造成锁表（逐行加锁，由于2PL，在commit的时候统一释放）。mqsql做了优化，会调用`unlock_row`把不满足条件的解锁,这也是为啥mysql常用隔离级别是RC不是RR，因为没有命中索引情况下，RR会锁表（防止有人新插入记录行，造成幻读），RC会锁行**

---
## 死锁
当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。
>进行死锁检测，即使只有100个线程事务，但死锁检测的复杂度是o(n^2)，会需要10000个数量级的检测，所以出现cpu消耗高，并发事务却没多少的情况

当出现死锁以后，有两种策略：
- 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数`innodb_lock_wait_timeout`来设置。
- 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数`innodb_deadlock_detect`设置为`on`，表示开启这个逻辑。
	- 一致性读不加锁，所以不需要死锁检测
	- 当访问的行上有锁时，需要检测，且不是检测所有事务，只检测有关事务，例如：B在等A，D在等C，新事务E发现需要等D，于是判断D与C会不会形成死锁。

> 怎么解决由热点行更新导致的性能问题呢？问题的症结在于，死锁检测要耗费大量的 CPU 资源。
> - 临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。
> - 控制并发度。比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法就是，在客户端做并发控制。但是，因为客户端很多，此办法会越来越不可行。因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的团队有能修改 MySQL 源码的人，也可以做在 MySQL 里面。基本思路就是，对于相同行的更新，在进入引擎之前排队。这样在 InnoDB 内部就不会有大量的死锁检测工作了。
> - 考虑通过将一行改成逻辑上的多行来减少锁冲突。还是以影院账户为例，可以考虑放在多条记录上，比如 10 个记录，影院的账户总额等于这 10 个记录的值的总和。这样每次要给影院账户加金额的时候，随机选其中一条记录来加。这样每次冲突概率变成原来的 1/10，可以减少锁等待个数，也就减少了死锁检测的 CPU 消耗。这个方案看上去是无损的，但其实这类方案需要根据业务逻辑做详细设计。如果账户余额可能会减少，比如退票逻辑，那么这时候就需要考虑当一部分行记录变成 0 的时候，代码要有特殊处理。


---

如果要删除一个表里面的前 10000 行数据，有以下三种方法可以做到：
- 第一种，直接执行`delete from T limit 10000`
- 第二种，在一个连接中循环执行 20 次 `delete from T limit 500`
- 第三种，在 20 个连接中同时执行`delete from T limit 500`。

第一种会造成长事务，第三种会造成锁竞争，可能导致死锁。
推荐使用第二种